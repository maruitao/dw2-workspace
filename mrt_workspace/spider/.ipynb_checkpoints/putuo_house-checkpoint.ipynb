{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# -*- coding:utf-8  -*-\n",
    "import urllib2\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import requests\n",
    "import time \n",
    "seed_url = r'http://sh.lianjia.com/zufang/putuo/d' #d1s7--d97s7\n",
    "SH_URL = r'sh.lianjia.com/'\n",
    "proxy_list=[\"60.207.239.247:3128\",\"218.201.98.196:3128\",\"60.207.239.245:3128\",\"103.235.245.35:8080\",\"125.161.109.204:8080\"]\n",
    "proxies = {\n",
    "  \"http\": \"http://60.207.239.247:3128\",\n",
    "  \"http\": \"http://218.201.98.196:3128\",\n",
    "  \"http\":\"125.161.109.204:8080\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def download(url , num_retries=2):\n",
    "    print 'downloading: ' , url \n",
    "#     \"Mozilla/5.0 (X11; Linux i686) AppleWebKit/535.7 (KHTML, like Gecko) Ubuntu/11.04 Chromium/16.0.912.77 Chrome/16.0.912.77 Safari/535.7\"\n",
    "#     'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/45.0.2454.101 Safari/537.36'\n",
    "    headers = {'User-agent':\"Mozilla/5.0 (X11; Linux i686) AppleWebKit/535.7 (KHTML, like Gecko) Ubuntu/11.04 Chromium/16.0.912.77 Chrome/16.0.912.77 Safari/535.7\"}\n",
    "    request = urllib2.Request(url, headers=headers)\n",
    "    try:\n",
    "        con = urllib2.urlopen(request)\n",
    "        html = con.read()\n",
    "        con.close()\n",
    "    except urllib2.URLError as e:\n",
    "        print 'Download error:', e.reason\n",
    "        html = None\n",
    "        if num_retries > 0:\n",
    "            if hasattr(e, 'code') and 500 <= e.code < 600:\n",
    "                # 当服务器返回5xx错误时，一般是临时性的, 重新发出请求\n",
    "                print url + \" hasn't normal access, try again! \"\n",
    "                return download(url, num_retries -1 )\n",
    "            if e.code == 403 : # 服务器查出是spider程序，封禁ip  需换代理ip重新访问 \n",
    "#                 requests.get(url, proxies=proxies)\n",
    "                proxy_support = urllib2.ProxyHandler(proxies=proxies)\n",
    "                opener = urllib2.build_opener(proxy_support,headers=headers)\n",
    "                con = urllib2.urlopen(opener)\n",
    "                html = con.read()\n",
    "                con.close()\n",
    "    return html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def generate_PT_urls(url, start , end):\n",
    "    result_urls = []\n",
    "    for i  in xrange(start, end):\n",
    "        temp= \"\"\n",
    "        temp = url + str(i) + 's7'\n",
    "        result_urls.append(temp)\n",
    "    return result_urls  # pass test!\n",
    "# generate_PT_urls(seed_url , 1, 97)  # 最后一页默认省去"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def get_data_by_page(html):\n",
    "    df = pd.DataFrame(columns=['maintitle','where_muroto_size','price','subway',\n",
    "                               'how_many_people_seen','putaway_time','link','floor_orie_locate'])\n",
    "    soup = BeautifulSoup(html.decode('utf8','ignore'), 'lxml')\n",
    "    \n",
    "    # 防止ip被封禁，采用隐式等待：即当页面元素加载全部完成时才进行下一步操作 \n",
    "    wait1.until(lambda driver: driver.find_element_by_xpath(\"//div[@id='link-report']/span\"))\n",
    "    \n",
    "    # 显式等待，随机sleep一小段时间\n",
    "    time.sleep(random.uniform(3,5)) \n",
    "    \n",
    "    # 抽取标题\n",
    "    listtemp = []\n",
    "    temp = soup.find_all('div',class_='info-panel')\n",
    "    for val in temp:\n",
    "        littemp.append(val.h2.a.get_text().strip())\n",
    "    df['maintitle'] = listtemp\n",
    "    \n",
    "    #抽取楼层floor  朝向orientation  位置locate \n",
    "    listtemp = []\n",
    "    temp = soup.find_all('div',class_='con')\n",
    "    for val in temp:\n",
    "        listtemp.append(val.get_text().strip())\n",
    "    df['floor_orie_locate'] = listtemp\n",
    "    \n",
    "    #抽取 小区  户室  大小\n",
    "    listtemp = []\n",
    "    temp = soup.find_all('div',class_='where')\n",
    "    for val in temp:\n",
    "        listtemp.append(val.get_text().strip())\n",
    "    df['where_muroto_size'] = listtemp\n",
    "    \n",
    "    #抽取 价格 \n",
    "    listtemp = []\n",
    "    temp = soup.find_all('div', class_='price')\n",
    "    for val in temp:\n",
    "        listtemp.append(val.get_text().strip())\n",
    "    df['price'] = listtemp\n",
    "    \n",
    "    #抽取 时间\n",
    "    listtemp = []\n",
    "    temp = soup.find_all('div', class_='price-pre')\n",
    "    for val in temp:\n",
    "        listtemp.append(val.get_text().strip())\n",
    "    df['putaway_time'] = listtemp\n",
    "    \n",
    "    #抽取 多少人观看\n",
    "    listtemp = []\n",
    "    temp = soup.find_all('div', class_='square')\n",
    "    for val in temp:\n",
    "        listtemp.append(val.get_text().strip())\n",
    "    df['how_many_people_seen'] = listtemp\n",
    "    \n",
    "    #抽取 url\n",
    "    listtemp = []\n",
    "    temp = soup.find_all('a', class_='js_triggerGray js_fanglist_title')\n",
    "    for val in temp:\n",
    "        listtemp.append(SH_URL + val.href)\n",
    "    df['link'] = listtemp\n",
    "    \n",
    "    #抽取 地铁位置\n",
    "    listtemp = []\n",
    "    temp = soup.find_all('span', class_='fang-subway-ex')\n",
    "    for val in temp:\n",
    "        listtemp.append(temp.get_text().strip())\n",
    "    df['subway'] = listtemp\n",
    "    \n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if __name__ ==\"__main__\":\n",
    "    final_df = pd.DataFrame(columns=['maintitle','where_muroto_size','price','subway',\n",
    "                               'how_many_people_seen','putaway_time','link','floor_orie_locate'])\n",
    "    all_urls = generate_PT_urls(seed_url, 1, 5)  # start=1  end=97，不包括end   最后一页end 不爬\n",
    "    pd.set_option('max_colwidth', 100)\n",
    "    \n",
    "    i = 1\n",
    "    for the_link in all_urls:\n",
    "        temp_df = get_data_by_page(download(the_link))\n",
    "        final_df = pd.concat([final_df, temp_df],axis=0 , ignore_index=True)\n",
    "        print \"第\", i , \" 页数据已爬取\"\n",
    "        i += 1\n",
    "\n",
    "    print \"开始写入csv\"\n",
    "    final_df.to_csv('lianjia.csv',encoding='utf-8')\n",
    "    print \"写入data.csv完成！！\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
