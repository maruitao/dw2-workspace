{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  本程序比较了sklearn中不同集成学习算法\n",
    "1. Bagging (Base Classifier： Decision Tree)\n",
    "2. RandomForest\n",
    "3. ExtraTrees\n",
    "4. Boosting\n",
    " - 4.1 AdaBoost (Base Classifier： Decision Tree)\n",
    " - 4.2 Stochastic Gradient Boosting  (Base Classifier： Decision Tree)\n",
    "5. Voting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import model_selection\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier  # GBDT\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import math as ma\n",
    "\n",
    "def get_dataset_by_uci(url, names):\n",
    "    \"\"\" 从url中读取csv文件\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(url , names=names)\n",
    "    array = df.values\n",
    "    X = array[: , 0:8]\n",
    "    Y = array[:, 8]\n",
    "    return X , Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = r'https://archive.ics.uci.edu/ml/machine-learning-databases/pima-indians-diabetes/pima-indians-diabetes.data'\n",
    "names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
    "X, Y = get_dataset_by_uci(url, names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77.0745\n"
     ]
    }
   ],
   "source": [
    "# 1.Bagged Decision Trees for Classification\n",
    "\n",
    "seed = 7\n",
    "kfold = model_selection.KFold(n_splits= 10 , random_state=seed) # 10折交叉验证\n",
    "cart = DecisionTreeClassifier()\n",
    "num_trees = 100   \n",
    "model = BaggingClassifier(base_estimator=cart, n_estimators=num_trees, random_state=seed)\n",
    "results = model_selection.cross_val_score(model, X, Y, cv=kfold)\n",
    "print ('%0.4f' %(results.mean() * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75.9057\n"
     ]
    }
   ],
   "source": [
    "#  2.RandomForest Classification\n",
    "\n",
    "seed = 7\n",
    "num_trees = 100\n",
    "max_features = round(ma.sqrt(len(names)))  # max_features = 3\n",
    "kfold = model_selection.KFold(n_splits=10, random_state=seed)\n",
    "model = RandomForestClassifier(n_estimators=num_trees, max_features=max_features)\n",
    "results = model_selection.cross_val_score(model, X, Y, cv=kfold)\n",
    "print ('%0.4f' %(results.mean() * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "76.1671\n"
     ]
    }
   ],
   "source": [
    "#  3.Extra Trees Classification\n",
    "\n",
    "seed = 7 \n",
    "num_trees = 100\n",
    "max_features = 7 \n",
    "kfold = model_selection.KFold(n_splits=10, random_state=seed)\n",
    "model = ExtraTreesClassifier(n_estimators=num_trees, max_features=max_features)\n",
    "results = model_selection.cross_val_score(model, X, Y, cv=kfold)\n",
    "print ('%0.4f' %(results.mean() * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "76.0458\n"
     ]
    }
   ],
   "source": [
    "#  4.1 AdaBoost\n",
    "\n",
    "seed = 7\n",
    "num_trees = 30\n",
    "kfold = model_selection.KFold(n_splits=10, random_state=seed)\n",
    "model = AdaBoostClassifier(n_estimators=num_trees, random_state=seed)\n",
    "results = model_selection.cross_val_score(model, X, Y, cv=kfold)\n",
    "print ('%0.4f' %(results.mean() * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "76.5584\n"
     ]
    }
   ],
   "source": [
    "#  4.2 Stochastic Gradient Boosting Classification\n",
    "\n",
    "seed = 7\n",
    "num_trees = 100\n",
    "kfold = model_selection.KFold(n_splits=10, random_state=seed)\n",
    "model = GradientBoostingClassifier()\n",
    "results = model_selection.cross_val_score(model, X, Y, cv=kfold)\n",
    "print ('%0.4f' %(results.mean() * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73.8158\n"
     ]
    }
   ],
   "source": [
    "#  5. Voting\n",
    "\n",
    "seed = 7\n",
    "kfold = model_selection.KFold(n_splits=10, random_state=seed)\n",
    "\n",
    "# create the  sub  models\n",
    "estimators = []\n",
    "model1 = LogisticRegression()\n",
    "estimators.append(('logistic', model1))\n",
    "model2 = DecisionTreeClassifier()\n",
    "estimators.append(('cart', model2))\n",
    "model3 = SVC()\n",
    "estimators.append(('svm', model3))\n",
    "\n",
    "# create the ensemble model\n",
    "ensemble = VotingClassifier(estimators=estimators)\n",
    "results = model_selection.cross_val_score(ensemble, X, Y, cv=kfold)\n",
    "print ('%.4f' %(results.mean() * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "####  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bagging\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "bagging = BaggingClassifier(KNeighborsClassifier(), max_samples=0.5 , max_features=0.5)\n",
    "#  Bagging方法可以传入不同的基分类器，  需指定： max_samples= 最大采样比 ， 最大特征比  max_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "X, Y = make_blobs(n_samples=10000, n_features=10, centers=100, random_state=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.979408793821\n"
     ]
    }
   ],
   "source": [
    "# 决策树的最大深度 = None 表示不设置最大深度\n",
    "# min_sampltes_split=1 , 当样本类别是唯一一类时\n",
    "#  min_samples_split must be an integer greater than 1 or a float in (0.0, 1.0]; got the integer 1\n",
    "\n",
    "# Decision Tree\n",
    "\n",
    "clf = DecisionTreeClassifier(max_depth=None, min_samples_split=2 , random_state=0)\n",
    "score = cross_val_score(clf, X, Y)\n",
    "print (score.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  min_samples_split 参数初始化问题\n",
    "####  诸多模型在构造时都有min_samples_split参数， 如RF  DT \n",
    "####  该参数的初始化要求 ：  integer greater than 1 or a float in (0.0, 1.0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.975831847891\n"
     ]
    }
   ],
   "source": [
    "# RandomForest  多个采样集分别训练多个随机树\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=10, max_depth=None, min_samples_split=0.05, random_state=0)\n",
    "score = cross_val_score(clf, X, Y)\n",
    "print(score.mean())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.999898989899\n"
     ]
    }
   ],
   "source": [
    "# ExtraTreesClassifier  从总体样本中训练多个随机树\n",
    "\n",
    "clf = ExtraTreesClassifier(n_estimators=10, max_depth=None, min_samples_split=0.2, random_state=0)\n",
    "score = cross_val_score(clf , X, Y)\n",
    "print (score.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.959967320261\n"
     ]
    }
   ],
   "source": [
    "#  AdaBoost\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "iris = load_iris()\n",
    "clf = AdaBoostClassifier(n_estimators=100)\n",
    "X = iris.data\n",
    "Y = iris.target\n",
    "scores = cross_val_score(clf, X, Y)\n",
    "print(scores.mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.91300000000000003"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#   Gradient Boosted Regression Trees (GBRT)\n",
    "from sklearn.datasets import make_hastie_10_2\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "X, y = make_hastie_10_2(random_state=0)\n",
    "X_train, X_test = X[: 2000], X[ 2000:]\n",
    "y_train, y_test = y[: 2000], y[2000:]\n",
    "\n",
    "clf = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0, max_depth=1, random_state=0).fit(X_train, y_train)\n",
    "clf.score(X_test, y_test)\n",
    "\n",
    "\n",
    "# 弱分类器的数量由参数n_estimators分类;每棵树的大小要么由 max_depth设定，要么由max_leaf_nodes设定。\n",
    "# learning_rate是一个取值范围为(0.0, 1.0]  通过shrinkage控制过拟合。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.0091548599603213"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Regression  GBRT  回归预测\n",
    "# GradientBoostingRegressor假设指定许多不同的损失函数； 回归问题默认的损失函数是最小二乘least squares ('ls')。\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.datasets import  make_friedman1\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "X, y = make_friedman1(n_samples=1200, random_state=0, noise=1.0)\n",
    "X_train, X_test = X[: 200], X[200 :]\n",
    "y_train, y_test = y[: 200] , y[200 :]\n",
    "\n",
    "est = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=1, random_state=0, loss='ls').fit(X_train, y_train)\n",
    "mean_squared_error(y_test, est.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.5.4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
